{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7125064,"sourceType":"datasetVersion","datasetId":4110115}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# About Dataset\nThis project will be based on the Coimbra Breast Cancer dataset. The data set consists of 116 observations, \nwhere 64 patients have breast cancer and 52 patients are a control group. The main purpose of this data is to build a predictive model,\nbut this project will mainly focus on PCA and see if it can help to get better results when combined with the KNN algorithm and comparisons on several neighbors and also get insights about data.\n\n# The dataset consists of 10 variables:\n\n* Age (years): the age of the individuals.\n* BMI (kg/m2): Body mass index, body fat index.\n* Glucose (mg/dL): Blood glucose levels.\n* Insulin (µU/mL): levels of insulin, a hormone linked to glucose regulation.\n* HOMA: assessment of a homeostatic model for insulin resistance and beta-cell function.\n* Leptin (ng/mL): Leptin levels, a hormone involved in appetite and energy balance.\n* Adiponectin (μg/ml): levels of adiponectin, a protein associated with metabolic regulation.\n* Resistin (ng/mL): levels of Resistin, a protein involved in insulin resistance.\n* MCP-1 (pg/dL): Monocyte Chemoattractant Protein-1, a cytokine involved in inflammation.\n\n# tags:\n1. Health controls\n2. Patients with breast cancer\n\n# In this project we will:\n* Investigating the data before performing tests\n* Attempting to reproduce the article we read with the KNN algorithm \n* PCA on the data\n* Principal Component Analysis (PCA)\n* component analysis\n* KNN on several different PCA components\n* conclusions\n\n# Data Analysis:\n# Principal component analysis (PCA)\nPrincipal component analysis (PCA) is a statistical method for dimensionality reduction. PCA is mainly used in multidimensional data set with lots of variables. The main goal is to create a simpler version of such a dataset. After this step the data is represented by the components that represent most of the variance from the original data set, with usually the first few components explaining the largest part of the variance.\n\n# K-Nearest Neighbors\nK-Nearest Neighbors is a supervised learning algorithm. When the data is 'trained' with data points corresponding to their classification. Once a point is predicted, it takes into account the points closest to it to determine its classification.\n\nBy performing KNN on the data obtained by running PCA we hope to get better results than the article we studied","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"13ab6f01-710f-4ea2-b102-8ee2f631eacb","_cell_guid":"e3fd26b3-9d79-447f-8318-909c7fc20dd3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:45:23.020156Z","iopub.execute_input":"2024-04-08T09:45:23.021094Z","iopub.status.idle":"2024-04-08T09:45:24.189030Z","shell.execute_reply.started":"2024-04-08T09:45:23.021060Z","shell.execute_reply":"2024-04-08T09:45:24.188150Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/coimbra-breastcancer/Coimbra_breast_cancer_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"# import the libraries:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_auc_score, precision_score, confusion_matrix, classification_report ,accuracy_score, recall_score, f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom tabulate import tabulate\n\n","metadata":{"_uuid":"9b40bf47-9038-46b9-ba40-d71da6af3297","_cell_guid":"bff703ae-a8bf-42ff-9157-9a0e98037318","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:45:24.190869Z","iopub.execute_input":"2024-04-08T09:45:24.191544Z","iopub.status.idle":"2024-04-08T09:45:26.337403Z","shell.execute_reply.started":"2024-04-08T09:45:24.191514Z","shell.execute_reply":"2024-04-08T09:45:26.336204Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/coimbra-breastcancer/Coimbra_breast_cancer_dataset.csv')\ndf.head()","metadata":{"_uuid":"7158e799-ac3b-451c-986a-35a48713fc07","_cell_guid":"50ab98ef-e7f6-4e76-95ea-81f382cfbc80","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:45:26.338975Z","iopub.execute_input":"2024-04-08T09:45:26.339293Z","iopub.status.idle":"2024-04-08T09:45:26.381236Z","shell.execute_reply.started":"2024-04-08T09:45:26.339268Z","shell.execute_reply":"2024-04-08T09:45:26.380012Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Age        BMI  Glucose  Insulin      HOMA   Leptin  Adiponectin  Resistin  \\\n0   48  23.500000       70    2.707  0.467409   8.8071     9.702400   7.99585   \n1   83  20.690495       92    3.115  0.706897   8.8438     5.429285   4.06405   \n2   82  23.124670       91    4.498  1.009651  17.9393    22.432040   9.27715   \n3   68  21.367521       77    3.226  0.612725   9.8827     7.169560  12.76600   \n4   86  21.111111       92    3.549  0.805386   6.6994     4.819240  10.57635   \n\n     MCP.1  Classification  \n0  417.114               1  \n1  468.786               1  \n2  554.697               1  \n3  928.220               1  \n4  773.920               1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>BMI</th>\n      <th>Glucose</th>\n      <th>Insulin</th>\n      <th>HOMA</th>\n      <th>Leptin</th>\n      <th>Adiponectin</th>\n      <th>Resistin</th>\n      <th>MCP.1</th>\n      <th>Classification</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>48</td>\n      <td>23.500000</td>\n      <td>70</td>\n      <td>2.707</td>\n      <td>0.467409</td>\n      <td>8.8071</td>\n      <td>9.702400</td>\n      <td>7.99585</td>\n      <td>417.114</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>83</td>\n      <td>20.690495</td>\n      <td>92</td>\n      <td>3.115</td>\n      <td>0.706897</td>\n      <td>8.8438</td>\n      <td>5.429285</td>\n      <td>4.06405</td>\n      <td>468.786</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>82</td>\n      <td>23.124670</td>\n      <td>91</td>\n      <td>4.498</td>\n      <td>1.009651</td>\n      <td>17.9393</td>\n      <td>22.432040</td>\n      <td>9.27715</td>\n      <td>554.697</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>68</td>\n      <td>21.367521</td>\n      <td>77</td>\n      <td>3.226</td>\n      <td>0.612725</td>\n      <td>9.8827</td>\n      <td>7.169560</td>\n      <td>12.76600</td>\n      <td>928.220</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86</td>\n      <td>21.111111</td>\n      <td>92</td>\n      <td>3.549</td>\n      <td>0.805386</td>\n      <td>6.6994</td>\n      <td>4.819240</td>\n      <td>10.57635</td>\n      <td>773.920</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"# view info:\ndf.info()","metadata":{"_uuid":"cb8e6745-1bac-433e-b0c6-8be0eb80e842","_cell_guid":"4efb566c-0f19-496d-97d1-08968868def5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:45:26.384316Z","iopub.execute_input":"2024-04-08T09:45:26.385266Z","iopub.status.idle":"2024-04-08T09:45:26.420707Z","shell.execute_reply.started":"2024-04-08T09:45:26.385225Z","shell.execute_reply":"2024-04-08T09:45:26.419212Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 116 entries, 0 to 115\nData columns (total 10 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   Age             116 non-null    int64  \n 1   BMI             116 non-null    float64\n 2   Glucose         116 non-null    int64  \n 3   Insulin         116 non-null    float64\n 4   HOMA            116 non-null    float64\n 5   Leptin          116 non-null    float64\n 6   Adiponectin     116 non-null    float64\n 7   Resistin        116 non-null    float64\n 8   MCP.1           116 non-null    float64\n 9   Classification  116 non-null    int64  \ndtypes: float64(7), int64(3)\nmemory usage: 9.2 KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# creates a copy of a DataFrame","metadata":{}},{"cell_type":"code","source":"#identify and count missing values in a DataFrame\n#creates a copy of a DataFrame\ndf.isnull().sum()\ncdf = df.copy()","metadata":{"_uuid":"8ed8ab34-45bd-4c61-a775-46e7b107adc3","_cell_guid":"190a459c-b8a0-4e16-a9ad-01de5f8b808b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:45:26.422700Z","iopub.execute_input":"2024-04-08T09:45:26.423142Z","iopub.status.idle":"2024-04-08T09:45:26.434968Z","shell.execute_reply.started":"2024-04-08T09:45:26.423108Z","shell.execute_reply":"2024-04-08T09:45:26.432360Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"cdf.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:45:26.437648Z","iopub.execute_input":"2024-04-08T09:45:26.438416Z","iopub.status.idle":"2024-04-08T09:45:26.449886Z","shell.execute_reply.started":"2024-04-08T09:45:26.438373Z","shell.execute_reply":"2024-04-08T09:45:26.448530Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Index(['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin', 'Adiponectin',\n       'Resistin', 'MCP.1', 'Classification'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Overview of the Dataset using fast_eda() Function from fastead Module\n","metadata":{}},{"cell_type":"code","source":"!pip install fasteda\nfrom fasteda import fast_eda\n# quick overview of the dataset using fast_eda() function from fasteda module\nfast_eda(df)","metadata":{"_uuid":"e42e2b90-3a1e-456a-895d-f8e095d2c155","_cell_guid":"5fc578dd-abd6-4c74-9da9-07d9d4b5edba","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:45:26.451525Z","iopub.execute_input":"2024-04-08T09:45:26.452181Z","iopub.status.idle":"2024-04-08T09:47:57.243539Z","shell.execute_reply.started":"2024-04-08T09:45:26.452139Z","shell.execute_reply":"2024-04-08T09:47:57.239303Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d33d8b5e40>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/fasteda/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d33d8b6140>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/fasteda/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d33d8b63e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/fasteda/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d33d8b6590>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/fasteda/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x79d33d8b6740>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/fasteda/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement fasteda (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for fasteda\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install fasteda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfasteda\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fast_eda\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# quick overview of the dataset using fast_eda() function from fasteda module\u001b[39;00m\n\u001b[1;32m      4\u001b[0m fast_eda(df)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fasteda'"],"ename":"ModuleNotFoundError","evalue":"No module named 'fasteda'","output_type":"error"}]},{"cell_type":"code","source":"# check for the correlation of the data\n\nplt.figure(figsize=(10,8))\ncor = cdf.corr()\nsns.heatmap(cor, annot=True,\n            cmap=plt.cm.RdYlGn, vmin=-1, vmax=1)\nplt.show()\n\n# Visualize the correlation of features with the target column\ncorrelation_with_target = cdf.corr()['Classification'].sort_values(ascending=False)\n\n# Remove correlation with itself\ncorrelation_with_target = correlation_with_target.drop('Classification')\n\nplt.figure(figsize=(8, 6))\ncorrelation_with_target.plot(kind='bar')\nplt.title('Correlation with Target (Classification)')\nplt.xlabel('Features')\nplt.ylabel('Correlation')\nplt.show()","metadata":{"_uuid":"d2eed9e5-1326-4224-a5e2-87565cb24d7b","_cell_guid":"b59fec03-07da-4966-a77a-f05fde506fed","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:47:57.245098Z","iopub.status.idle":"2024-04-08T09:47:57.245619Z","shell.execute_reply.started":"2024-04-08T09:47:57.245400Z","shell.execute_reply":"2024-04-08T09:47:57.245424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(Glucose, HOMA, Insullin and Resistin) have high positive correlation with the 'classification' column and BMI have the highiest negative correlation\n\nas we can see we have high correlation between HOMA and Insulin feature\nSolution:\nwe can omit one of the features\n'but we can use PCA instead foe feature extraction in the following sections'","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, hue = 'Classification', palette = 'rainbow', diag_kind = \"hist\" );","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.247933Z","iopub.status.idle":"2024-04-08T09:47:57.248896Z","shell.execute_reply.started":"2024-04-08T09:47:57.248542Z","shell.execute_reply":"2024-04-08T09:47:57.248566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Currently there do not seem to be two features that separate the data well","metadata":{}},{"cell_type":"code","source":"cdf['Classification'] = cdf['Classification'].replace({1: 0, 2: 1})\ncdf['Classification'] = cdf['Classification'].replace({0: 1, 1: 0})\n\nX = cdf.drop('Classification',axis=1)\ny = cdf['Classification']","metadata":{"_uuid":"bcc404f4-4150-48d1-b9a7-9328ac8827ec","_cell_guid":"5bccda8c-9ada-48ba-a81e-437b16a13acc","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-04-08T09:47:57.250567Z","iopub.status.idle":"2024-04-08T09:47:57.251485Z","shell.execute_reply.started":"2024-04-08T09:47:57.251171Z","shell.execute_reply":"2024-04-08T09:47:57.251207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0 = negative\n\n1 = positive","metadata":{}},{"cell_type":"markdown","source":"# part 1 - Attempting to reproduce the article we read\nIn this section we will try to reproduce the results of the article. In the article, they used the same data set and the KNN algorithm with 7 neighbors on the pixels: resistin, glucose, age and BMI and got the following results:\n\n* Accuracy -> 87.50%\n* Sensitivity -> 84.62%\n* Specificity -> 91.00%\n* AUC -> 87.76\n* Confusion Matrix:\n\n[[ 11  1]\n\n [ 2 10]]","metadata":{}},{"cell_type":"code","source":"X_artcl= X[['Age', 'BMI', 'Glucose','Resistin']]\nX_train, X_test, y_train, y_test = train_test_split(X_artcl,y,test_size=0.2,random_state=42 ,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.253104Z","iopub.status.idle":"2024-04-08T09:47:57.254046Z","shell.execute_reply.started":"2024-04-08T09:47:57.253686Z","shell.execute_reply":"2024-04-08T09:47:57.253710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Define the number of neighbors\nn_neighbors = 7\n\n# Initialize the k-NN classifier with the specified number of neighbors and parallel processing\nknn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the classes for training and test data\ny_train_predicted = knn.predict(X_train)\ny_test_predicted = knn.predict(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.255614Z","iopub.status.idle":"2024-04-08T09:47:57.256506Z","shell.execute_reply.started":"2024-04-08T09:47:57.256225Z","shell.execute_reply":"2024-04-08T09:47:57.256250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef calculate_specificity(conf_matrix):\n    true_negatives = conf_matrix[0, 0]\n    false_positives = conf_matrix[0, 1]\n    specificity = true_negatives / (true_negatives + false_positives)\n    return specificity\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.257720Z","iopub.status.idle":"2024-04-08T09:47:57.258104Z","shell.execute_reply.started":"2024-04-08T09:47:57.257922Z","shell.execute_reply":"2024-04-08T09:47:57.257938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Calculate evaluation metrics for the training set\nknn_train_accuracy_score = accuracy_score(y_train, y_train_predicted)\nknn_train_precision_score = precision_score(y_train, y_train_predicted)\nknn_train_recall_score = recall_score(y_train, y_train_predicted)\nknn_train_f1_score = f1_score(y_train, y_train_predicted)\nknn_train_conf_matrix = confusion_matrix(y_train, y_train_predicted)\nknn_train_auc_score = roc_auc_score(y_train, y_train_predicted)\nknn_train_specificity_score = calculate_specificity(knn_train_conf_matrix)\n\n\n# Calculate evaluation metrics for the test set\nknn_test_accuracy_score = accuracy_score(y_test, y_test_predicted)\nknn_test_precision_score = precision_score(y_test, y_test_predicted)\nknn_test_recall_score = recall_score(y_test, y_test_predicted)\nknn_test_f1_score = f1_score(y_test, y_test_predicted)\nknn_test_conf_matrix = confusion_matrix(y_test, y_test_predicted)\nknn_test_auc_score = roc_auc_score(y_test, y_test_predicted)\nknn_test_specificity_score = calculate_specificity(knn_test_conf_matrix)\n\n\n# Print evaluation metrics for the training set\nprint(\"Training Set Evaluation Metrics:\")\nprint(\"Accuracy Score:\", knn_train_accuracy_score)\nprint(\"Precision Score:\", knn_train_precision_score)\nprint(\"Recall Score:\", knn_train_recall_score)\nprint(\"F1 Score:\", knn_train_f1_score)\nprint(\"AUC Score:\", knn_train_auc_score)\nprint(\"Specificity Score:\", knn_train_specificity_score)\nprint(\"Confusion Matrix:\")\nprint(knn_train_conf_matrix)\nprint(\"------------------------------------------------------\")\n\n# Print evaluation metrics for the test set\nprint(\"Test Set Evaluation Metrics:\")\nprint(\"Accuracy Score:\", knn_test_accuracy_score)\nprint(\"Precision Score:\", knn_test_precision_score)\nprint(\"Recall Score:\", knn_test_recall_score)\nprint(\"F1 Score:\", knn_test_f1_score)\nprint(\"AUC Score:\", knn_test_auc_score)\nprint(\"Specificity Score:\", knn_test_specificity_score)\nprint(\"Confusion Matrix:\")\nprint(knn_test_conf_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.260021Z","iopub.status.idle":"2024-04-08T09:47:57.260387Z","shell.execute_reply.started":"2024-04-08T09:47:57.260209Z","shell.execute_reply":"2024-04-08T09:47:57.260224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, our results are a little different from the results of the article.","metadata":{}},{"cell_type":"markdown","source":"# part 2 - Attempt to improve the results by performing PCA","metadata":{}},{"cell_type":"markdown","source":"**PCA** is a dimensionality reduction technique used to transform large datasets into smaller ones while retaining most of the original information. It works by creating new variables, called principal components, which are linear combinations of the original variables. These components capture the maximum variance in the data, making them more interpretable and easier to visualize. PCA helps simplify data analysis by reducing the number of variables, making it faster and more efficient for machine learning algorithms. While the principal components themselves may not have direct interpretability, they provide valuable insights into the underlying structure of the data.  PCA aims to preserve as much information as possible while reducing dimensionality, leading to improved data exploration and analysis.\n\n**The method works in five main stages:**\n1. Standardization: The range of values of each variable is adjusted so that they all have a mean of 0 and a variance of 1.\n2. The covariance matrix: calculated to identify relationships (correlations) between the variables.\n3. Eigenvectors and eigenvalues of the covariance matrix: calculated to find the principal components.\n4. Feature vector: decides which primary components to use.\n5. Transformation: the data is adjusted to the new axes created by the main components","metadata":{}},{"cell_type":"code","source":"\n# Define the features you want to analyze using PCA\nfeatures = ['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin', 'Adiponectin',\n            'Resistin', 'MCP.1']\n\n# Extract the features you defined earlier\nX_f = cdf[features]\n\n# Normalize the features to have a mean of 0 and a standard deviation of 1\n# This helps PCA work better with features on different scales\nscaler = StandardScaler()\nX_norm = scaler.fit_transform(X_f.to_numpy())\n\n# Perform PCA on the normalized features\n# This will reduce the dimensionality of your data while preserving most of the information\npca = PCA()\nX_pca = pca.fit_transform(X_norm)\n\n# Convert the PCA transformed data to a DataFrame for easier manipulation\nnames = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]  # Create column names for PCA components\nX_pcadf = pd.DataFrame(X_pca, columns=names)\n\n# Print the first few rows of the PCA DataFrame to see the transformed data\nprint(\"First few rows of the PCA DataFrame:\")\nprint(X_pcadf.head())\nprint(\"------------------------------------------------------------------------\")\n\n# Print the shape of the PCA DataFrame to see the new dimensionality\nprint(\"Shape of the PCA DataFrame:\")\nprint(X_pcadf.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.261669Z","iopub.status.idle":"2024-04-08T09:47:57.262090Z","shell.execute_reply.started":"2024-04-08T09:47:57.261893Z","shell.execute_reply":"2024-04-08T09:47:57.261910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X_pca,y,test_size=0.2,random_state=42 ,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.263233Z","iopub.status.idle":"2024-04-08T09:47:57.263585Z","shell.execute_reply.started":"2024-04-08T09:47:57.263409Z","shell.execute_reply":"2024-04-08T09:47:57.263424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We will run the PCA results on KNN with a different number of vandevador if there is an improvement in the results**","metadata":{}},{"cell_type":"code","source":"\n# Define the range of n_neighbors values to test\nn_neighbors_values = [3, 5, 7, 9]\n\n# Initialize lists to store evaluation metrics for each n_neighbors value\ntrain_accuracy_scores = []\ntest_accuracy_scores = []\ntrain_precision_scores = []\ntest_precision_scores = []\ntrain_recall_scores = []\ntest_recall_scores = []\ntrain_f1_scores = []\ntest_f1_scores = []\ntrain_conf_matrices = []\ntest_conf_matrices = []\n\n# Loop over different values of n_neighbors\nfor n_neighbors in n_neighbors_values:\n    # Create and fit the k-NN classifier\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n    knn.fit(X_train, y_train)\n    \n    # Predictions on training and test sets\n    y_train_predicted = knn.predict(X_train)\n    y_test_predicted = knn.predict(X_test)\n    \n    # Compute evaluation metrics for the training set\n    train_accuracy_scores.append(accuracy_score(y_train, y_train_predicted))\n    train_precision_scores.append(precision_score(y_train, y_train_predicted))\n    train_recall_scores.append(recall_score(y_train, y_train_predicted))\n    train_f1_scores.append(f1_score(y_train, y_train_predicted))\n    train_conf_matrices.append(confusion_matrix(y_train, y_train_predicted))\n    \n    # Compute evaluation metrics for the test set\n    test_accuracy_scores.append(accuracy_score(y_test, y_test_predicted))\n    test_precision_scores.append(precision_score(y_test, y_test_predicted))\n    test_recall_scores.append(recall_score(y_test, y_test_predicted))\n    test_f1_scores.append(f1_score(y_test, y_test_predicted))\n    test_conf_matrices.append(confusion_matrix(y_test, y_test_predicted))\n\n# Print evaluation metrics for each value of n_neighbors\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    print(f\"n_neighbors = {n_neighbors}:\")\n    print(\"Training Set Evaluation Metrics:\")\n    print(\"Accuracy Score:\", train_accuracy_scores[i])\n    print(\"Precision Score:\", train_precision_scores[i])\n    print(\"Recall Score:\", train_recall_scores[i])\n    print(\"F1 Score:\", train_f1_scores[i])\n    print(\"Confusion Matrix:\")\n    print(train_conf_matrices[i])\n    print(\"------------------------------------------------------\")\n    print(\"Test Set Evaluation Metrics:\")\n    print(\"Accuracy Score:\", test_accuracy_scores[i])\n    print(\"Precision Score:\", test_precision_scores[i])\n    print(\"Recall Score:\", test_recall_scores[i])\n    print(\"F1 Score:\", test_f1_scores[i])\n    print(\"Confusion Matrix:\")\n    print(test_conf_matrices[i])\n    print(\"======================================================\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.264782Z","iopub.status.idle":"2024-04-08T09:47:57.265161Z","shell.execute_reply.started":"2024-04-08T09:47:57.264978Z","shell.execute_reply":"2024-04-08T09:47:57.264993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, there is no improvement in the results for all the PCA components together, therefore we will investigate the PCA components and at the end we will try to run the test again on fewer components","metadata":{}},{"cell_type":"markdown","source":"# Analysis of components","metadata":{}},{"cell_type":"code","source":"# Print the singular values\nsingular_values=pca.singular_values_\nprint(\"Singular values of PCA:\", singular_values)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.266168Z","iopub.status.idle":"2024-04-08T09:47:57.266514Z","shell.execute_reply.started":"2024-04-08T09:47:57.266337Z","shell.execute_reply":"2024-04-08T09:47:57.266352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In PCA, singular values are the key to understanding data variance. They tell us how much information each principal component (PC) captures, with larger values indicating more important directions. By keeping the top k largest ones, we reduce data complexity while preserving key patterns. This analysis helps us choose the optimal number of PCs and identify dominant patterns in the data","metadata":{}},{"cell_type":"code","source":"# Compute the covariance matrix from the normalized data\ncov_matrix = np.cov(X_norm.T)\nprint(\"Convariance matrix: \", cov_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.268318Z","iopub.status.idle":"2024-04-08T09:47:57.268712Z","shell.execute_reply.started":"2024-04-08T09:47:57.268519Z","shell.execute_reply":"2024-04-08T09:47:57.268537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating the covariance matrix from the original data allows us to understand the relationships between the various variables in the data system. The covariance matrix provides information about the linear relationships between pairs of variables, and thus may help us identify the most significant features of the data.\n\nWhen we perform PCA, we look for the principal components that represent the maximum variance in the data set. Calculating the covariance matrix allows us to calculate the eigenvalues and eigenvectors of the matrix, which are exactly the main components we are looking for. Each eigenvalue represents the maximum variation in certain directions, and each eigenvector represents the way (or \"component\") in which the data varies in these directions.","metadata":{}},{"cell_type":"code","source":"# Calculate eigenvalues and eigenvectors of the covariance matrix\neigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\nprint(\"Eigenvectors:\", eigenvectors)\nprint(\"Eigenvalues:\", eigenvalues)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.271450Z","iopub.status.idle":"2024-04-08T09:47:57.271831Z","shell.execute_reply.started":"2024-04-08T09:47:57.271632Z","shell.execute_reply":"2024-04-08T09:47:57.271647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By calculating the eigenvectors and eigenvalues of the covariance matrix, we obtain the principal components of the data set. These principal components represent the directions of maximum variance in the data (the larger the eigenvalue, the greater the maximum variance in the directions, the more relevant they are to us), and are used to transform the original data into a new, lower dimensional space in PCA. The eigenvalues provide information on the amount of variance explained by each principal component","metadata":{}},{"cell_type":"code","source":"# Sort the eigenvalues and eigenvectors in descending order\neig_pairs = [(eigenvalues[index], eigenvectors[:, index]) for index in range(len(eigenvalues))]\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Print sorted eigenvalue-eigenvector pairs\nprint(\"Sorted eigenvalue-eigenvector pairs:\")\nprint(eig_pairs)\n\n# Extract the sorted eigenvalues and eigenvectors\neigenvalues_sorted = [pair[0] for pair in eig_pairs]\neigenvectors_sorted = [pair[1] for pair in eig_pairs]\n\n# Print sorted eigenvalues\nprint(\"Sorted eigenvalues:\", eigenvalues_sorted)","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.273005Z","iopub.status.idle":"2024-04-08T09:47:57.273450Z","shell.execute_reply.started":"2024-04-08T09:47:57.273261Z","shell.execute_reply":"2024-04-08T09:47:57.273278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sorting eigenvalues and eigenvectors in descending order of the eigenvalues allows us to identify the most important components of the data. When we represent the data in the space of the eigenvectors, the first components contain the most central variance of the data, and are therefore more important.\n\nUsing eigenvalue sorting, we organize the most central components of the data so that the first component will be the component containing the most central variance, the second component will be the component containing the second most variance, and so on. This allows us to efficiently identify the most important elements in the data set and translate them into display and analysis. ","metadata":{}},{"cell_type":"code","source":"# Plot explained variance ratio\nplt.figure(figsize=(9,10))\n\n# Individual explained variance\nplt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, alpha=0.5, label='Individual explained variance')\n\n# Cumulative explained variance\nplt.plot(range(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--', label='Cumulative explained variance')\n\n# Highlighting 85% explained variance threshold\nplt.axhline(y=0.85, color='r', linestyle='-', label='85% Explained Variance')\n\nplt.xlabel('Principal Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Explained Variance Ratio by Principal Components')\nplt.legend()\nplt.grid(True)\nplt.xlim(0, len(eigenvalues_sorted)+1)\nplt.ylim(0, 1.01)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.274592Z","iopub.status.idle":"2024-04-08T09:47:57.274993Z","shell.execute_reply.started":"2024-04-08T09:47:57.274766Z","shell.execute_reply":"2024-04-08T09:47:57.274781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe code calculates the variance explained by each principal component in a PCA analysis and visualizes it using bar and step plots. It first computes the total variance by summing the eigenvalues of the covariance matrix. Then, it calculates the proportion of variance explained by each principal component and computes the cumulative explained variance. The bar plot illustrates the individual variance explained by each principal component, while the step plot shows the cumulative explained variance as more components are added. These visualizations aid in identifying the principal components that capture the most variance and determining the number of components needed to retain a significant portion of the total variance, crucial for dimensionality reduction in PCA analysis.\n(In PCA, we typically aim to retain a sufficient number of principal components to capture a high percentage of the total variance (e.g., 80-95%)).\n\nThe dashed line shows the quantitative sum of the information relations of the components up to the current component. It shows the quantitative sum of the variance in the data that was transformed by the previous components, and the remaining variance that remains to be transformed by the current component.","metadata":{}},{"cell_type":"code","source":"# Get the explained variance for each principal component\nev = pca.explained_variance_\nprint(ev)\n\n# Plot the explained variance for each feature using seaborn lineplot\nplt.figure(figsize=(8, 6))\nsns.lineplot(x=np.array(X_pcadf.columns), y=ev)\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"% Explained variance\")\nplt.title(\"Explained Variance by Features\")\nplt.ylim(0, 4)  # Adjust y-axis limit for better visualization\nplt.xticks(rotation=45)  # Rotate x-axis labels for better visibility\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.275923Z","iopub.status.idle":"2024-04-08T09:47:57.276818Z","shell.execute_reply.started":"2024-04-08T09:47:57.276596Z","shell.execute_reply":"2024-04-08T09:47:57.276614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explained variance is a measure of how much of the total variance in the original dataset is explained by each principal component. The explained variance of a principal component is equal to the eigenvalue associated with that component.\n\nIn Sklearn PCA, the explained variance of each principal component can be accessed through the explained_variance_ attribute. For example, if pca is a Sklearn PCA object, pca.explained_variance_[i] gives the explained variance of the i-th principal component.\n\nThe total explained variance of a set of principal components is simply the sum of the explained variance of those components.\n\nPrincipal components with higher variance (or explained variance ratio) are considered more important as they capture more information from the original dataset. Components with lower variance may contain less relevant information and may be discarded if the goal is to reduce dimensionality while preserving most of the variability in the data.","metadata":{}},{"cell_type":"code","source":"# Calculate the cumulative explained variance\nevc = np.cumsum(pca.explained_variance_)\nprint(evc)\n\n# Plot the cumulative explained variance for principal components using seaborn lineplot\nplt.figure(figsize=(8, 6))\nsns.lineplot(x=np.array(X_pcadf.columns), y=evc)\nplt.xlabel(\"Principal Components\")  # Label for x-axis\nplt.ylabel(\"Cumulative Explained Variance\")  # Label for y-axis\nplt.title(\"Cumulative Explained Variance by Features\")  # Title of the plot\nplt.ylim(0, 9.2)  # Set y-axis limit for better visualization\nplt.xticks(rotation=45)  # Rotate x-axis labels for better readability\nplt.tight_layout()  # Adjust layout to prevent labels from being cut off\nplt.show()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.277932Z","iopub.status.idle":"2024-04-08T09:47:57.278309Z","shell.execute_reply.started":"2024-04-08T09:47:57.278131Z","shell.execute_reply":"2024-04-08T09:47:57.278146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot visualizes the cumulative explained variance by the principal components across the pc.\nIt helps understand how much of the total variance in the dataset is explained cumulatively as more principal components are considered.\nEach point on the line represents the cumulative explained variance when including a certain number of principal components.","metadata":{}},{"cell_type":"code","source":"# Create a DataFrame to store the loadings of each feature on each principal component\n# Loadings represent the correlations between original features and principal components\nloadings = pd.DataFrame(pca.components_, index=names, columns=np.array(features))\n\n# Display the loadings DataFrame\nprint(\"Loadings of Features on Principal Components:\")\nloadings","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.279502Z","iopub.status.idle":"2024-04-08T09:47:57.281819Z","shell.execute_reply.started":"2024-04-08T09:47:57.281505Z","shell.execute_reply":"2024-04-08T09:47:57.281530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"represent the correlations between the original features and the PCs, indicating how much each feature contributes to each PC.","metadata":{}},{"cell_type":"code","source":"# covariance matrix of principal components\npca.get_covariance()","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.282884Z","iopub.status.idle":"2024-04-08T09:47:57.283401Z","shell.execute_reply.started":"2024-04-08T09:47:57.283137Z","shell.execute_reply":"2024-04-08T09:47:57.283158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The covariance matrix of principal components in PCA encapsulates the covariance structure among the orthogonal vectors derived from the original features of the dataset. Each element in this matrix represents the covariance between different principal components, where diagonal elements signify the variance of individual principal components and off-diagonal elements indicate their covariance. Analyzing this matrix offers insights into how principal components are related to each other and aids in understanding the variability captured by each component. Overall, the covariance matrix of principal components serves as a tool in interpreting the transformed feature space obtained through PCA.","metadata":{}},{"cell_type":"code","source":"\n\n# Define the features\nfeatures = ['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin', 'Adiponectin', 'Resistin', 'MCP.1']\n\n# Extract the features\nX_f = cdf[features]\ny = cdf['Classification']  # Assuming 'Classification' is the target variable\n\n# Normalize the features\nscaler = StandardScaler()\nX_norm = scaler.fit_transform(X_f.to_numpy())\n\n# Perform PCA\npca = PCA(n_components=5).fit(X_f)\nX_pca = pca.fit_transform(X_norm)\n\n\n# Define the number of top principal components to consider\nnum_components = 5\n\n# Define the range of neighbors in KNN\nneighbors_range = range(3, 9)\n\n# Convert the PCA transformed data to a DataFrame for easier manipulation\nnames = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]  # Create column names for PCA components\nX_pcadf = pd.DataFrame(X_pca, columns=names)\n\n# Lists to store evaluation metrics\naccuracy_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\nauc_scores = []\nconf_matrices = []\nspecificity_scores = []\n\n# Loop over each number of components from 1 to num_components\nfor n in range(1, num_components + 1):\n    # Select the top n principal components\n    X_pca_subset = X_pca[:, :n]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_pca_subset, y, test_size=0.2, random_state=42)\n\n    # Lists to store evaluation metrics for different number of neighbors\n    accuracy_scores_n = []\n    precision_scores_n = []\n    recall_scores_n = []\n    f1_scores_n = []\n    auc_scores_n = []\n    conf_matrices_n = []\n    specificity_scores_n = []\n\n\n\n# Loop over each number of components from 1 to num_components\nfor n in range(1, num_components + 1):\n    # Select the top n principal components\n    X_pca_subset = X_pca[:, :n]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_pca_subset, y, test_size=0.2, random_state=42)\n\n    # Lists to store evaluation metrics for different number of neighbors\n    accuracy_scores_n = []\n    precision_scores_n = []\n    recall_scores_n = []\n    f1_scores_n = []\n    auc_scores_n = []\n    conf_matrices_n = []\n    specificity_scores_n = []\n\n    # Loop over each number of neighbors\n    for k in neighbors_range:\n        # Train the KNN classifier\n        knn = KNeighborsClassifier(n_neighbors=k)\n        knn.fit(X_train, y_train)\n\n        # Predict the labels for the test set\n        y_pred = knn.predict(X_test)\n\n        # Calculate the evaluation metrics\n        accuracy = accuracy_score(y_test, y_pred)\n        accuracy_scores_n.append(accuracy)\n\n        precision = precision_score(y_test, y_pred)\n        precision_scores_n.append(precision)\n\n        recall = recall_score(y_test, y_pred)\n        recall_scores_n.append(recall)\n\n        f1 = f1_score(y_test, y_pred)\n        f1_scores_n.append(f1)\n\n        auc = roc_auc_score(y_test, y_pred)  # Calculate AUC\n        auc_scores_n.append(auc)\n\n        conf_matrix = confusion_matrix(y_test, y_pred)\n        conf_matrices_n.append(conf_matrix)\n        \n        specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])  # Calculate Specificity\n        specificity_scores_n.append(specificity)\n\n    # Store the evaluation metrics for the best number of neighbors\n    best_idx = np.argmax(accuracy_scores_n)\n    accuracy_scores.append(accuracy_scores_n[best_idx])\n    precision_scores.append(precision_scores_n[best_idx])\n    recall_scores.append(recall_scores_n[best_idx])\n    f1_scores.append(f1_scores_n[best_idx])\n    auc_scores.append(auc_scores_n[best_idx])\n    conf_matrices.append(conf_matrices_n[best_idx])\n    specificity_scores.append(specificity_scores_n[best_idx])\n\n    best_k = neighbors_range[best_idx]  # Get the best number of neighbors\n    print(f\"Accuracy with {n} principal components and {best_k} neighbors: {accuracy_scores_n[best_idx]:.4f}\")\n\n# Plotting the accuracy vs number of principal components\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, num_components + 1), accuracy_scores, marker='o')\nplt.title('Accuracy vs Number of Principal Components')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Accuracy')\nplt.xticks(range(1, num_components + 1))\nplt.grid(True)\nplt.show()\n\n# Print evaluation metrics for the test set\nfor n in range(num_components):\n    print(f\"Test Set Evaluation Metrics with {n + 1} Principal Components:\")\n    print(\"Accuracy Score:\", accuracy_scores[n])\n    print(\"Precision Score:\", precision_scores[n])\n    print(\"Recall Score:\", recall_scores[n])\n    print(\"F1 Score:\", f1_scores[n])\n    print(\"AUC Score:\", auc_scores[n])\n    print(\"Specificity:\", specificity_scores[n])\n    print(\"Confusion Matrix:\")\n    print(conf_matrices[n])\n    print(\"--------------------------------\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.285045Z","iopub.status.idle":"2024-04-08T09:47:57.285411Z","shell.execute_reply.started":"2024-04-08T09:47:57.285230Z","shell.execute_reply":"2024-04-08T09:47:57.285245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code performs a classification task using k-Nearest Neighbors (KNN) algorithm with different numbers of principal components obtained through PCA. Here's an explanation of the results:\n\nThe plot shows how the accuracy of the KNN classifier changes as the number of principal components increases.\nGenerally, we expect the accuracy to increase as we include more principal components because they capture more variance in the data, providing more information for classification.\nHowever, there may be a point where adding more components does not significantly improve accuracy, or even reduces it due to noise or overfitting.\n\n* The amount of components can be explained because in machine learning, increasing the number of features beyond a certain point can lead to performance degradation due to the curse of dimensionality. This phenomenon occurs because as the number of dimensions increases, the volume of space increases exponentially, making the data sparse. This sparsity makes it difficult to accurately estimate classified parameters and affects the performance of distance-based algorithms.\n\n* The value of K can be explained as follows: since there is no built-in method for finding the best value for K. Values of K are tested for data size with the rule of thumb being that the maximum K is k = sqrt(N ) where \"N\" represents the number of samples in a data set Your training, and the value of K is odd. Therefore for us the value of K is 1-10 .In addition it is known that choosing smaller values for K can be noisy and affect the result more.And larger values of K will have smoother decision boundaries, meaning lower variance but increased bias.\n\nThe results demonstrate the effectiveness of PCA in dimensionality reduction and its impact on classification performance.\n","metadata":{}},{"cell_type":"code","source":"\n# Extract the first two principal components\npc1 = X_pca[:, 0]\npc2 = X_pca[:, 1]\n\n# Plotting the first two principal components\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(pc1, pc2, c=y, cmap='viridis')\nplt.title('Scatter Plot of First Two Principal Components')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\n\n# Add legend\nlegend1 = plt.legend(*scatter.legend_elements(),\n                    title=\"Classification\",\n                    loc=\"upper right\")\nplt.gca().add_artist(legend1)\n\nplt.grid(True)\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.286902Z","iopub.status.idle":"2024-04-08T09:47:57.287574Z","shell.execute_reply.started":"2024-04-08T09:47:57.287384Z","shell.execute_reply":"2024-04-08T09:47:57.287401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the graph shows a scatter plot of the first two principal components extracted using PCA from the input data. In the plot, each point represents one sample from the data, where the color of the point indicates its corresponding category or class based on the values of y.","metadata":{}},{"cell_type":"markdown","source":"# add matrix of the resolt","metadata":{}},{"cell_type":"code","source":"\n# Define the number of top principal components to consider\nnum_components = 5\n\n# Define the range of n_neighbors values to test\nn_neighbors_values = [3, 5, 7, 9]\n\n# Create a matrix to store accuracy scores\naccuracy_matrix = np.zeros((num_components, len(n_neighbors_values)))\n\n# Loop over each number of components from 1 to num_components\nfor n in range(1, num_components + 1):\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_pca[:, :n], y, test_size=0.2, random_state=42)\n    \n    # Loop over different values of n_neighbors\n    for i, n_neighbors in enumerate(n_neighbors_values):\n        # Create and fit the k-NN classifier\n        knn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n        knn.fit(X_train, y_train)\n\n        # Evaluate the model on the test set\n        accuracy = knn.score(X_test, y_test)\n\n        # Store accuracy in the matrix\n        accuracy_matrix[n - 1, i] = accuracy\n\n# Print the accuracy matrix with component and neighbor labels\nprint(\"Accuracy Matrix:\")\nheaders = [\"Components/Neighbors\"] + [f\"{n_neighbors} Neighbors\" for n_neighbors in n_neighbors_values]\ntable_data = []\nfor n in range(1, num_components + 1):\n    row = [f\"{n} Components\"]\n    for i, n_neighbors in enumerate(n_neighbors_values):\n        row.append(f\"{accuracy_matrix[n - 1, i]:.4f}\")\n    table_data.append(row)\n\nprint(tabulate(table_data, headers=headers, tablefmt=\"pretty\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T09:47:57.289235Z","iopub.status.idle":"2024-04-08T09:47:57.289756Z","shell.execute_reply.started":"2024-04-08T09:47:57.289491Z","shell.execute_reply":"2024-04-08T09:47:57.289513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This table shows the accuracy scores achieved by the k-NN classifier for each combination of the number of principal components and the number of neighbors.\n\nInterpretation of the results:\n\n* When only one component is used, the accuracy is relatively low for all values of k (number of neighbors). This is expected because a single component may not capture enough information to accurately classify the data.\n* As the number of components increases (from 2 to 5), the accuracy generally improves, especially for lower values of k (eg, 3 or 5 neighbors).\n* With more components, the classifier has access to more information about the data structure, leading to better classification performance.\n* However, using too many components can also introduce noise or irrelevant information, which can reduce performance. This is evident in the slight drop in accuracy when going from 4 to 5 components.\n* The choice of the number of neighbors (k) also affects the classification accuracy. In some cases, increasing the number of neighbors leads to better performance, while in others, a smaller number of neighbors performs better.\n\nSummary:\n\nThe results demonstrate the importance of feature extraction (using PCA) and parameter tuning (such as choosing the appropriate number of neighbors) in achieving good classification performance.\nIt is essential to find a balance between the amount of information stored (number of components) and the complexity of the model (number of neighbors) to avoid overfitting or underfitting the data.","metadata":{}},{"cell_type":"markdown","source":"# Results of each component for each number of neighbors","metadata":{}},{"cell_type":"code","source":"\n# Define the features\nfeatures = ['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin', 'Adiponectin', 'Resistin', 'MCP.1']\n\n# Extract the features\nX_f = cdf[features]\ny = cdf['Classification']  # Assuming 'Classification' is the target variable\n\n# Normalize the features\nscaler = StandardScaler()\nX_norm = scaler.fit_transform(X_f)\n\n# Perform PCA\npca = PCA()\nX_pca = pca.fit_transform(X_norm)\n\n# Define the number of top principal components to consider\nnum_components = 5\n\n# Define the range of n_neighbors values to test\nn_neighbors_values = [3, 5, 7, 9]\n\n# Loop over each number of components from 1 to num_components\nfor n in range(1, num_components + 1):\n    # Select the top n principal components\n    X_pca_subset = X_pca[:, :n]\n\n    # Split the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X_pca_subset, y, test_size=0.2, random_state=42)\n    \n    # Loop over different values of n_neighbors\n    for n_neighbors in n_neighbors_values:\n        # Create and fit the k-NN classifier\n        knn = KNeighborsClassifier(n_neighbors=n_neighbors, n_jobs=-1)\n        knn.fit(X_train, y_train)\n\n        # Predictions on training and test sets\n        y_train_predicted = knn.predict(X_train)\n        y_test_predicted = knn.predict(X_test)\n\n        # Compute evaluation metrics for the training set\n        train_accuracy_score = accuracy_score(y_train, y_train_predicted)\n        train_precision_score = precision_score(y_train, y_train_predicted)\n        train_recall_score = recall_score(y_train, y_train_predicted)\n        train_f1_score = f1_score(y_train, y_train_predicted)\n        train_conf_matrix = confusion_matrix(y_train, y_train_predicted)\n\n        # Compute evaluation metrics for the test set\n        test_accuracy_score = accuracy_score(y_test, y_test_predicted)\n        test_precision_score = precision_score(y_test, y_test_predicted)\n        test_recall_score = recall_score(y_test, y_test_predicted)\n        test_f1_score = f1_score(y_test, y_test_predicted)\n        test_conf_matrix = confusion_matrix(y_test, y_test_predicted)\n\n        # Print evaluation metrics\n        print(f\"n_neighbors = {n_neighbors}, Principal Components = {n}:\")\n        print(\"Training Set Evaluation Metrics:\")\n        print(\"Accuracy Score:\", train_accuracy_score)\n        print(\"Precision Score:\", train_precision_score)\n        print(\"Recall Score:\", train_recall_score)\n        print(\"F1 Score:\", train_f1_score)\n        print(\"Confusion Matrix:\")\n        print(train_conf_matrix)\n        print(\"------------------------------------------------------\")\n        print(\"Test Set Evaluation Metrics:\")\n        print(\"Accuracy Score:\", test_accuracy_score)\n        print(\"Precision Score:\", test_precision_score)\n        print(\"Recall Score:\", test_recall_score)\n        print(\"F1 Score:\", test_f1_score)\n        print(\"Confusion Matrix:\")\n        print(test_conf_matrix)\n        print(\"======================================================\")\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-04-08T09:47:57.291370Z","iopub.status.idle":"2024-04-08T09:47:57.292257Z","shell.execute_reply.started":"2024-04-08T09:47:57.291989Z","shell.execute_reply":"2024-04-08T09:47:57.292011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results\n\nAfter performing PCA and running the obtained data on the KNN algorithm, it was found that the best result was obtained with 2 Principal Components and with 5 neighbors. By Reduction the additional features we get a simpler system that yields good results\n\n**To conclude, the results we received are:**\nTest Set Evaluation Metrics with 2 Principal Components:\n* Accuracy Score: 0.875\n* Precision Score: 0.909\n* Recall Score: 0.833\n* F1 Score: 0.869\n* AUC Score: 0.875\n* Specificity: 0.916\n* Confusion Matrix:\n\n[[11  1]\n\n [ 2 10]]\n \nSurprisingly, these are also the results obtained in the article. This means that we were able to compare the results and even improve them because we predicted the same level of accuracy with a smaller dimension (smaller amount of components)\n\n\n# conclusions\n\n* The results of the KNN algorithm with 7 neighbors and 4 features indicate its effectiveness in predicting breast cancer, with an accuracy of 83%. Although the results are a little lower than those of the original article (87.5%), they are still very high and prove the efficiency of the algorithm.\n*  Choosing the number of PC components and the number of neighbors are important parameters in the classification process in the KNN algorithm. The results show that there is an increase and decrease exactly according to the change in the number of components and the number of neighbors.\n* The results of the KNN algorithm after performing PCA show that the use of the Dimensionality Reduction technique by PCA can lead to an improvement in performance. Especially when using a small number of PC components.\n* The small differences between the results of the article and our results show the ability of the model to maintain high efficiency and accuracy, even after performing PCA. This indicates the importance of the exact choice of parameters and the ability of the model to identify the most important details in the data.\n* The results obtained highlight the potential of machine learning algorithms, and in particular KNN, for predicting breast cancer. Optimum parameter selection, use of \"Dimensionality Reduction\" techniques such as PCA, and selection of relevant features are key factors for achieving high accuracy and optimal performance.","metadata":{}},{"cell_type":"markdown","source":"# sources\nWe have used this notebook in a number of Kagle brochures, materials from various websites on the Internet, most of which are attached here:\n\nhttps://saturncloud.io/blog/what-is-sklearn-pca-explained-variance-and-explained-variance-ratio-difference/\n\nhttps://sebastianraschka.com/Articles/2015_pca_in_3_steps.html\n\nhttps://builtin.com/data-science/step-step-explanation-principal-component-analysis\n\nhttps://www.kaggle.com/code/avikumart/pca-principal-component-analysis-from-scratch\n\nhttps://rstudio-pubs-static.s3.amazonaws.com/471974_9c6250108efc497789ee5840f24b0db4.html#data-analysis\n\nhttps://rpubs.com/KAndruszek/471974\n\nhttps://erdogant.github.io/pca/pages/html/Algorithm.html#normalizing-out-pcs\n\nhttps://www.kaggle.com/code/abdelrasoul/knn-pca-ml-83\n\nhttps://www.kaggle.com/code/tanshihjen/module-eda-with-fasteda\n\nhttps://www.kaggle.com/code/saswattulo/coimbra-breastcancer-prediction-with-91-6-accurac\n\nhttps://www.kaggle.com/code/benyaminghahremani/coimbra-breast-cancer-classification-feature-eng\n\nhttps://medium.com/analytics-vidhya/dimensionality-reduction-principal-component-analysis-d1402b58feb1","metadata":{}}]}